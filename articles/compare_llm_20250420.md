---
title: "時代の進歩についていけないのでLLMを比較した"
emoji: "⚖️"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["LLM","ChatGPT","Claude","Gemini"]
published: false
---

## 商用提供中のLLMモデル一覧（2025年4月19日時点）

| モデル名 | 提供元 | 提供形態 | 提供開始日 | 備考 |
| --- | --- | --- | --- | --- |
| **GPT‑3.5 Turbo** | OpenAI | API、ChatGPT（Web UI） | 2023年3月1日（API公開） ([OpenAI launches an API for ChatGPT, plus dedicated capacity for enterprise customers | TechCrunch](https://techcrunch.com/2023/03/01/openai-launches-an-api-for-chatgpt-plus-dedicated-capacity-for-enterprise-customers/#:~:text=Kyle%20Wiggers)) ([OpenAI launches an API for ChatGPT, plus dedicated capacity for enterprise customers | TechCrunch](https://techcrunch.com/2023/03/01/openai-launches-an-api-for-chatgpt-plus-dedicated-capacity-for-enterprise-customers/#:~:text=Priced%20at%20%240,are%20among%20the%20early%20adopters)) | ChatGPTのベースモデル。GPT-3を改良した大規模言語モデルで、高い応答性能を持つ。安価（$0.002/1kトークン）で提供され、ChatGPT無料版やAPI経由で広く利用可能 ([OpenAI launches an API for ChatGPT, plus dedicated capacity for enterprise customers | TechCrunch](https://techcrunch.com/2023/03/01/openai-launches-an-api-for-chatgpt-plus-dedicated-capacity-for-enterprise-customers/#:~:text=Kyle%20Wiggers)) ([OpenAI launches an API for ChatGPT, plus dedicated capacity for enterprise customers | TechCrunch](https://techcrunch.com/2023/03/01/openai-launches-an-api-for-chatgpt-plus-dedicated-capacity-for-enterprise-customers/#:~:text=Priced%20at%20%240,are%20among%20the%20early%20adopters))。 |
| **GPT‑4** | OpenAI | API、ChatGPT Plus（Web UI） | 2023年3月14日 ([GPT-4 - Wikipedia](https://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT%20foundation%20models.,training)) | GPT-3.5の後継となる多目的高性能モデル。8k/32kトークンの長い文脈に対応し、信頼性・創造性が向上 ([GPT-4 - Wikipedia](https://en.wikipedia.org/wiki/GPT-4#:~:text=OpenAI%20stated%20that%20GPT,17%20%5D%20It%20can)) ([GPT-4 - Wikipedia](https://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT%20foundation%20models.,training))。ChatGPT Plusで公開され、プロ向けAPIも提供中。マルチモーダル入力（画像）にも対応（Vision機能、2023年11月追加） ([GPT-4 - Wikipedia](https://en.wikipedia.org/wiki/GPT-4#:~:text=In%20November%202023%2C%20OpenAI%20announced,23))。 |
| **GPT‑4 Turbo（Vision）** | OpenAI | API、ChatGPT Plus | 2023年11月6日 ([GPT-4 - Wikipedia](https://en.wikipedia.org/wiki/GPT-4#:~:text=In%20November%202023%2C%20OpenAI%20announced,23)) | GPT-4の高速・拡張版。最大128Kトークンの超長文脈に対応し、価格も大幅に低廉化 ([GPT-4 - Wikipedia](https://en.wikipedia.org/wiki/GPT-4#:~:text=In%20November%202023%2C%20OpenAI%20announced,23))。画像入力も可能（Vision版）で、ChatGPTやAPIで提供。従来のGPT-4と比べ応答速度が向上した。 |
| **GPT‑4o** (“omni”) | OpenAI | API、ChatGPT無料版/Plus | 2024年5月13日 ([GPT-4 - Wikipedia](https://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,and%20generating%20outputs%20across%20text)) | テキスト・画像・音声の統合マルチモーダル対応モデル ([GPT-4 - Wikipedia](https://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,and%20generating%20outputs%20across%20text))。多言語性能や視覚・音声理解が強化され、会話応答速度も人間並みに高速化。従来モデルよりコスト効率・処理効率が向上しており ([GPT-4 - Wikipedia](https://en.wikipedia.org/wiki/GPT-4#:~:text=audio%2C%20and%20image%20modalities%20in,24))、ChatGPT全ユーザに画像・テキスト機能を即時展開（音声機能はPlus限定） ([GPT-4 - Wikipedia](https://en.wikipedia.org/wiki/GPT-4#:~:text=OpenAI%20plans%20to%20immediately%20roll,25))。 |
| **GPT‑4o mini** | OpenAI | API | 2024年7月18日 ([GPT-4o mini: advancing cost-efficient intelligence | OpenAI](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/#:~:text=July%2018%2C%202024)) ([GPT-4o mini: advancing cost-efficient intelligence | OpenAI](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/#:~:text=OpenAI%20is%20committed%20to%20making,cheaper%20than%20GPT%E2%80%913.5%20Turbo)) | GPT-4oの軽量版（小型モデル）で、コスト効率に優れる ([GPT-4o mini: advancing cost-efficient intelligence | OpenAI](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/#:~:text=OpenAI%20is%20committed%20to%20making,cheaper%20than%20GPT%E2%80%913.5%20Turbo))。128Kトークン文脈に対応し、テキスト・画像入力（将来的に音声・動画も）をサポート ([GPT-4o mini: advancing cost-efficient intelligence | OpenAI](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/#:~:text=time%20text%20responses%20%28e,support%20chatbots))。推論速度が高速で**大幅低価格**（入力100万トークンあたり15¢） ([GPT-4o mini: advancing cost-efficient intelligence | OpenAI](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/#:~:text=with%20AI%20by%20making%20intelligence,cheaper%20than%20GPT%E2%80%913.5%20Turbo))。中規模タスクや高並列呼び出しに適し、GPT-3.5 Turbo等より高性能 ([GPT-4o mini: advancing cost-efficient intelligence | OpenAI](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/#:~:text=GPT%E2%80%914o%20mini%20has%20been%20evaluated,across%20several%20key%20benchmarks%202))。 |
| **GPT‑4.1** | OpenAI | API | 2025年4月14日 ([Introducing GPT-4.1 in the API | OpenAI](https://openai.com/index/gpt-4-1/#:~:text=Share)) | GPT-4シリーズの最新モデル。コード生成・指示追従性能が大幅向上し、最大100万トークンのコンテキストを扱える ([Introducing GPT-4.1 in the API | OpenAI](https://openai.com/index/gpt-4-1/#:~:text=Share))。GPT-4oを上回る精度を達成しつつ**低コスト**で提供される。知識データも2024年6月まで更新済み ([Introducing GPT-4.1 in the API | OpenAI](https://openai.com/index/gpt-4-1/#:~:text=Share))。 |
| **GPT‑4.1 mini** | OpenAI | API | 2025年4月14日 ([Introducing GPT-4.1 in the API | OpenAI](https://openai.com/index/gpt-4-1/#:~:text=Share)) | GPT-4.1の小型版。GPT-4o miniの後継で、高性能と低遅延のバランスを実現したモデル。応答レイテンシを抑えつつ、GPT-4.1相当の高度な指示応答やコーディングを多くの用途に提供。 |
| **GPT‑4.1 nano** | OpenAI | API | 2025年4月14日 ([Introducing GPT-4.1 in the API | OpenAI](https://openai.com/index/gpt-4-1/#:~:text=Share)) | OpenAI初の「ナノ」モデルで、最小規模のGPT-4.1ファミリ。リアルタイム性が特に要求される用途向けに最適化された超軽量モデル。性能は上位モデルより劣るが、非常に高速で低コストな推論が可能。 |
| **Claude 3 Opus** | Anthropic | API | 2024年2月（推定） | Anthropicの第3世代における最大モデル（Opus）。**高精度・高性能**で、複雑な分析や大規模なステップを要するタスクに対応 ([Build with Claude \ Anthropic](https://www.anthropic.com/api#:~:text=Opus))。出力品質はClaude 3系中最高だが、トークン当たりのコストも最も高い（1M入力あたり$15） ([Claude 3.5 Haiku](https://simonwillison.net/2024/Nov/4/haiku/#:~:text=Given%20that%20Anthropic%20claim%20that,it%E2%80%99s%20a%20small%20surprise%20nonetheless)) ([Claude 3.5 Haiku](https://simonwillison.net/2024/Nov/4/haiku/#:~:text=Gemini%201.5%20Flash,00))。長文応答（100kトークン以上）にも対応。 |
| **Claude 3 Haiku** | Anthropic | API、Claude.ai（Web UI） | 2024年3月（推定） | 第3世代Claudeの**軽量高速モデル**（Haiku）。画像入力対応の安価なモデルで、当時Anthropicモデル中最廉価 ([Claude 3.5 Haiku](https://simonwillison.net/2024/Nov/4/haiku/#:~:text=Claude%203,Anthropic%20model%20for%20handling%20those))。応答は高速だが精度は上位モデルに劣る。Claude 3.5 Haiku登場前は画像付きタスクで最適な選択肢だった ([Claude 3.5 Haiku](https://simonwillison.net/2024/Nov/4/haiku/#:~:text=I%20was%20expecting%20this%20to,while%20maintaining%20the%20same%20pricing))。 |
| **Claude 3.5 Sonnet** (Claude 3.7) | Anthropic | API、Claude.ai | 2024年6月20日（v3.5）<br>2025年2月17日（v3.7） ([Grok (chatbot) - Wikipedia](https://en.wikipedia.org/wiki/Grok_(chatbot)#:~:text=match%20at%20L387%20Grok,2%20Type)) ([Grok (chatbot) - Wikipedia](https://en.wikipedia.org/wiki/Grok_(chatbot)#:~:text=On%20February%2017%2C%202025%2C%20xAI,3%20was%20trained%20with)) | Claude第3.5世代の高性能モデル（Sonnet）。2024年後半の3.5版から2025年2月の3.7版へのアップデートでコード実行能力など大幅強化 ([Build with Claude \ Anthropic](https://www.anthropic.com/api#:~:text=Coding))。**長文対応**（8kトークン出力）と高い推論精度を両立し、Anthropicの主力モデルとなっている。安全性・多言語対応も強化。 |
| **Claude 3.5 Haiku** | Anthropic | API、Claude.ai、他社クラウド（Bedrock等） | 2024年11月4日 ([Claude 3.5 Haiku](https://simonwillison.net/2024/Nov/4/haiku/#:~:text=4th%20November%202024)) | Anthropic最新世代の**最速モデル**。推論速度が極めて速く、コーディングやツール利用能力も向上 ([Anthropic’s Claude 3.5 Haiku model now available in Amazon Bedrock - AWS](https://aws.amazon.com/about-aws/whats-new/2024/11/anthropics-claude-3-5-haiku-model-amazon-bedrock#:~:text=Anthropic%E2%80%99s%20Claude%203,on%20many%20intelligence%20benchmarks%E2%80%94including%20coding))。Claude 3 Opus（前世代最大モデル）に匹敵する知能を**低コスト**で発揮するため、価格も従来Haiku比で引き上げられた ([Claude 3.5 Haiku](https://simonwillison.net/2024/Nov/4/haiku/#:~:text=,reflect%20its%20increase%20in%20intelligence))。現在テキスト入力のみ対応（画像入力は将来対応予定） ([Anthropic’s Claude 3.5 Haiku model now available in Amazon Bedrock - AWS](https://aws.amazon.com/about-aws/whats-new/2024/11/anthropics-claude-3-5-haiku-model-amazon-bedrock#:~:text=help%20with%20use%20cases%20such,for%20image%20inputs%20to%20follow))。 |
| **Gemini 1.5 Flash** | Google (DeepMind) | API（Gemini API）、Google AI Studio、Gemini アプリ | 2024年5月10日（Google I/O 2024） ([
            
            Gemini 1.5 Flash-8B is now production ready
            
            
            - Google Developers Blog
            
        ](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/#:~:text=At%20I%2FO%2C%20we%20announced%20Gemini,the%20limits%20of%20what%E2%80%99s%20possible)) | Gemini 1.5世代の**軽量高速モデル**。マルチモーダル（テキスト・画像・音声・動画）入力対応で、大規模文脈（最大100万トークン）も処理可能 ([Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra](https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/#:~:text=Both%201,and%20to%20Google%20Cloud%20customers))。高頻度・高ボリュームの推論を低レイテンシ・低コストで提供するよう最適化されている ([Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra](https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/#:~:text=While%20it%E2%80%99s%20a%20lighter%20weight,impressive%20quality%20for%20its%20size))。要約やチャット、映像説明など幅広いタスクに適し、1.5 Proより軽量だが十分な汎用性能を持つ ([Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra](https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/#:~:text=While%20it%E2%80%99s%20a%20lighter%20weight,impressive%20quality%20for%20its%20size))。 |
| **Gemini 1.5 Flash‑8B** | Google (DeepMind) | API、AI Studio | 2024年10月3日 ([
            
            Gemini 1.5 Flash-8B is now production ready
            
            
            - Google Developers Blog
            
        ](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/#:~:text=Gemini%201.5%20Flash,production%20ready)) ([
            
            Gemini 1.5 Flash-8B is now production ready
            
            
            - Google Developers Blog
            
        ](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/#:~:text=Today%2C%20Gemini%201.5%20Flash,ready%20and%20comes%20with)) | Gemini 1.5 Flashのさらなる小型モデル（約80億パラメータ） ([Gemini models | Gemini API | Google AI for Developers](https://ai.google.dev/gemini-api/docs/models#:~:text=Gemini%201.5%20Flash%20gemini,8B))。1.5 Flash比で**低価格（半額）・低遅延**を実現し、小規模プロンプトでの応答速度が速い ([
            
            Gemini 1.5 Flash-8B is now production ready
            
            
            - Google Developers Blog
            
        ](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/#:~:text=Today%2C%20Gemini%201.5%20Flash,ready%20and%20comes%20with))。性能は1.5 Flashと同等レベルまで最適化されており、チャットや長文翻訳・音声書き起こしなど高ボリューム用途で極めてコスト効率が高い ([
            
            Gemini 1.5 Flash-8B is now production ready
            
            
            - Google Developers Blog
            
        ](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/#:~:text=Last%20month%2C%20we%20released%20an,and%20long%20context%20language%20translation))。 |
| **Gemini 1.5 Pro** | Google (DeepMind) | API、AI Studio、Gemini アプリ | 2024年3月（推定） ([Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra](https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/#:~:text=In%20December%2C%20we%20launched%20our,window%20of%201%20million%20tokens)) | Gemini 1.0 Ultraを基に性能強化したモデル。**高精度な推論**と広範なマルチモーダル推論能力を持ち、最大100万〜200万トークンの超長文コンテキストも扱える（APIでは2Mトークンまでプレビュー提供） ([Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra](https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/#:~:text=three%20sizes%3A%20Ultra%2C%20Pro%20and,window%20of%201%20million%20tokens)) ([Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra](https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/#:~:text=Both%201,and%20to%20Google%20Cloud%20customers))。複雑なコード生成や推論タスクに適し、Flashモデルより応答は遅いが高度な推論が可能。 |
| **Gemini 2.0 Flash** | Google (DeepMind) | API、AI Studio、Gemini アプリ | 2024年12月（発表）<br>2025年1月31日（安定版） ([Gemini 2.0 Flash AI Model is Now Stable, Replacing 1.5 Pro](https://www.themobileindian.com/news/gemini-2-0-flash-ai-model-is-now-stable-replacing-1-5-pro-in-gemini-app#:~:text=The%20Google%20Gemini%202,Google%20in%20a%20blog%20post)) | Gemini第2世代の高速モデル。1.5 Proの後継として**より高速かつ高性能**になり、日常的な文章生成やブレインストーミング等で優れた性能を示す ([Gemini 2.0 Flash AI Model is Now Stable, Replacing 1.5 Pro](https://www.themobileindian.com/news/gemini-2-0-flash-ai-model-is-now-stable-replacing-1-5-pro-in-gemini-app#:~:text=The%20Google%20Gemini%202,Google%20in%20a%20blog%20post))。Geminiアプリでは既存の1.5 Proモデルと置き換えられた ([Gemini 2.0 Flash AI Model is Now Stable, Replacing 1.5 Pro](https://www.themobileindian.com/news/gemini-2-0-flash-ai-model-is-now-stable-replacing-1-5-pro-in-gemini-app#:~:text=The%20Google%20Gemini%202,Google%20in%20a%20blog%20post))。マルチモーダル対応に加え、超長文入力や複雑なタスクにも高精度に対処できる。 |
| **Gemini 2.0 Flash‑Lite** | Google (DeepMind) | API、AI Studio | 2024年12月（発表） | Gemini 2.0 Flashの軽量版。**低コスト・低遅延**に特化したモデルで、大規模モデルのサブセットとして高頻度リクエストを高速処理する用途に向く ([Gemini models  |  Gemini API  |  Google AI for Developers](https://ai.google.dev/gemini-api/docs/models#:~:text=Gemini%202.0%20Flash,volume%20and%20lower%20intelligence%20tasks))。Flashより性能は抑えられるが、簡易なチャットボットや大量データ処理でコスト効率良く運用可能。 |
| **Gemini 2.0 Flash Live** | Google (DeepMind) | API、AI Studio | 2025年初頭（推定） | Gemini 2.0世代のリアルタイム対話モデル。音声や動画入力に対し**双方向対話**を低遅延で実現するよう設計されている ([Gemini models  |  Gemini API  |  Google AI for Developers](https://ai.google.dev/gemini-api/docs/models#:~:text=Imagen%203%20%20%60imagen,bidirectional%20voice%20and%20video%20interactions))。音声対話エージェントやリアルタイム音声アシスタント向けに提供され、ストリーミング応答や対話継続性に優れる。 |
| **Gemini 2.5 Flash** *(Preview)* | Google (DeepMind) | API（プレビュー）、AI Studio | 2025年4月17日（プレビュー公開） ([Gemini models  |  Gemini API  |  Google AI for Developers](https://ai.google.dev/gemini-api/docs/models#:~:text=Model%20variant%20Input,generation%20features%2C%20speed%2C%20thinking%2C%20realtime)) | Gemini第2.5世代の最新軽量モデル（試験提供版）。**価格性能比が最高**とされ、マルチモーダル長文推論能力とコスト効率を両立 ([Gemini models  |  Gemini API  |  Google AI for Developers](https://ai.google.dev/gemini-api/docs/models#:~:text=2)) ([Gemini models  |  Gemini API  |  Google AI for Developers](https://ai.google.dev/gemini-api/docs/models#:~:text=Model%20variant%20Input,experimental))。開発者向けに限定プレビューされており、次世代機能や性能改善を先行提供。 |
| **Gemini 2.5 Pro** *(Preview)* | Google (DeepMind) | API（プレビュー） | 2025年3月25日（プレビュー公開） ([Gemini models  |  Gemini API  |  Google AI for Developers](https://ai.google.dev/gemini-api/docs/models#:~:text=%60gemini,Lite)) | Gemini第2.5世代の高機能モデル（試験提供版）。**高度な推論・コード生成**やマルチモーダル理解に優れた最先端モデルで、Flash版より大規模・高精度 ([Gemini models  |  Gemini API  |  Google AI for Developers](https://ai.google.dev/gemini-api/docs/models#:~:text=thinking%2C%20cost%20efficiency%20Gemini%202,Lite))。開発者プレビューで提供中（一部制限あり）で、難解な問題解決や高度なエージェント用途に適用可能。 |
| **DeepSeek-R1** | DeepSeek (杭州深度求索) | モバイルアプリ（iOS/Android）、オープンウェイト（MITライセンス） | 2025年1月20日 ([DeepSeek - Wikipedia](https://en.wikipedia.org/wiki/DeepSeek#:~:text=On%2020%20January%202025%2C%20DeepSeek,drop%20in%20Nvidia%27s%20share)) | 中国・DeepSeek社の初の公開LLMモデル。67Bパラメータのモデルで、独自チャットボットとして一般公開 ([DeepSeek - Wikipedia](https://en.wikipedia.org/wiki/DeepSeek#:~:text=On%2020%20January%202025%2C%20DeepSeek,drop%20in%20Nvidia%27s%20share))。MITライセンスでモデル重みを公開し、商用利用も可能。「GPT-4に匹敵する応答性能」を謳いながらも訓練コストを約600万ドルと大幅に削減して開発された（効率的なMoE技術を活用） ([DeepSeek - Wikipedia](https://en.wikipedia.org/wiki/DeepSeek#:~:text=The%20DeepSeek,significantly%20lower%20than%20other%20LLMs))。低コスト戦略で提供され、モバイルアプリでは公開直後に米国iOS無料アプリで最多DLとなるなど急速に普及 ([DeepSeek - Wikipedia](https://en.wikipedia.org/wiki/DeepSeek#:~:text=On%2020%20January%202025%2C%20DeepSeek,drop%20in%20Nvidia%27s%20share))。 |
| **Grok 3** | xAI (Elon Musk) | X（Twitter）上のBot（有料会員向け）、xAI公式Web・アプリ | 2025年2月17日 ([Grok (chatbot) - Wikipedia](https://en.wikipedia.org/wiki/Grok_(chatbot)#:~:text=match%20at%20L387%20Grok,2%20Type)) ([Grok (chatbot) - Wikipedia](https://en.wikipedia.org/wiki/Grok_(chatbot)#:~:text=On%20February%2017%2C%202025%2C%20xAI,3%20was%20trained%20with)) | Elon MuskのxAI社によるチャットボットLLM「Grok」の第3世代モデル ([Grok (chatbot) - Wikipedia](https://en.wikipedia.org/wiki/Grok_(chatbot)#:~:text=On%20February%2017%2C%202025%2C%20xAI,3%20was%20trained%20with))。最新のフラッグシップモデルで、推論能力が大幅に向上。X（旧Twitter）の**プレミアムプラス会員向け**に提供されており ([Grok (chatbot) - Wikipedia](https://en.wikipedia.org/wiki/Grok_(chatbot)#:~:text=Initially%2C%20access%20to%20Grok,41))、「Think」モードで高度な推論（チェインオブソート）も可能。テキストベースだが、近く音声モードなどマルチモーダル機能も追加予定とされる ([Grok (chatbot) - Wikipedia](https://en.wikipedia.org/wiki/Grok_(chatbot)#:~:text=match%20at%20L429%20Initially%2C%20access,41))。 |
| **Grok 3 Mini** | xAI (Elon Musk) | X（Premium会員向け） | 2025年2月17日 ([Grok (chatbot) - Wikipedia](https://en.wikipedia.org/wiki/Grok_(chatbot)#:~:text=On%20February%2017%2C%202025%2C%20xAI,3%20was%20trained%20with)) ([Grok (chatbot) - Wikipedia](https://en.wikipedia.org/wiki/Grok_(chatbot)#:~:text=xAI%20also%20released%20Grok,41)) | Grok-3と同時公開された**高速軽量版**モデル ([Grok (chatbot) - Wikipedia](https://en.wikipedia.org/wiki/Grok_(chatbot)#:~:text=On%20February%2017%2C%202025%2C%20xAI,3%20was%20trained%20with))。応答速度を優先し一部精度を犠牲にした設計で、よりリーズナブルな利用が可能 ([Grok (chatbot) - Wikipedia](https://en.wikipedia.org/wiki/Grok_(chatbot)#:~:text=xAI%20also%20released%20Grok,41))。Xの通常Premium（有料）ユーザ向けに提供されており、日常的な質問応答でキビキビした対話ができる。一方で高度な推論や正確性は大型モデルのGrok-3に劣る。将来的に企業向けAPI提供も計画されている ([Grok (chatbot) - Wikipedia](https://en.wikipedia.org/wiki/Grok_(chatbot)#:~:text=Initially%2C%20access%20to%20Grok,41))。 |

＜出典＞

 ([OpenAI launches an API for ChatGPT, plus dedicated capacity for enterprise customers | TechCrunch](https://techcrunch.com/2023/03/01/openai-launches-an-api-for-chatgpt-plus-dedicated-capacity-for-enterprise-customers/#:~:text=Kyle%20Wiggers)) ([OpenAI launches an API for ChatGPT, plus dedicated capacity for enterprise customers | TechCrunch](https://techcrunch.com/2023/03/01/openai-launches-an-api-for-chatgpt-plus-dedicated-capacity-for-enterprise-customers/#:~:text=Priced%20at%20%240,are%20among%20the%20early%20adopters))OpenAI (2023) *“OpenAI launches an API for ChatGPT…”*（TechCrunch）  
 ([GPT-4 - Wikipedia](https://en.wikipedia.org/wiki/GPT-4#:~:text=In%20November%202023%2C%20OpenAI%20announced,23))Wikipedia (2023) “GPT-4 – OpenAI announced GPT-4 Turbo…”  
 ([GPT-4 - Wikipedia](https://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,and%20generating%20outputs%20across%20text)) ([GPT-4 - Wikipedia](https://en.wikipedia.org/wiki/GPT-4#:~:text=audio%2C%20and%20image%20modalities%20in,24))Wikipedia (2024) “GPT-4o – introduced by OpenAI…”  
 ([GPT-4o mini: advancing cost-efficient intelligence | OpenAI](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/#:~:text=OpenAI%20is%20committed%20to%20making,cheaper%20than%20GPT%E2%80%913.5%20Turbo))OpenAI (2024) *“GPT-4o mini: advancing cost-efficient intelligence”*  
 ([Introducing GPT-4.1 in the API | OpenAI](https://openai.com/index/gpt-4-1/#:~:text=Share))OpenAI (2025) *“Introducing GPT-4.1 in the API”*  
 ([Anthropic’s Claude 3.5 Haiku model now available in Amazon Bedrock - AWS](https://aws.amazon.com/about-aws/whats-new/2024/11/anthropics-claude-3-5-haiku-model-amazon-bedrock#:~:text=Anthropic%E2%80%99s%20Claude%203,on%20many%20intelligence%20benchmarks%E2%80%94including%20coding))AWS (2024) *“Claude 3.5 Haiku model now available in Amazon Bedrock”*  
 ([Claude 3.5 Haiku](https://simonwillison.net/2024/Nov/4/haiku/#:~:text=,reflect%20its%20increase%20in%20intelligence))Simon Willison (2024) “Claude 3.5 Haiku” （価格引き上げに関するコメント）  
 ([Build with Claude \ Anthropic](https://www.anthropic.com/api#:~:text=Coding))Anthropic (2025) *“Build with Claude – Claude 3.7 Sonnet can run code”*  
 ([Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra](https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/#:~:text=While%20it%E2%80%99s%20a%20lighter%20weight,impressive%20quality%20for%20its%20size))Google DeepMind (2024) *“Gemini 1.5 Flash: optimized for speed and efficiency”* (Google Blog)  
 ([
            
            Gemini 1.5 Flash-8B is now production ready
            
            
            - Google Developers Blog
            
        ](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/#:~:text=Last%20month%2C%20we%20released%20an,and%20long%20context%20language%20translation))Google (2024) *“Gemini 1.5 Flash-8B is now production ready”* (Developers Blog)  
 ([Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra](https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/#:~:text=Both%201,and%20to%20Google%20Cloud%20customers))Google DeepMind (2024) *“Gemini 1.5 Pro…1M token context window”* (Google Blog)  
 ([Gemini 2.0 Flash AI Model is Now Stable, Replacing 1.5 Pro](https://www.themobileindian.com/news/gemini-2-0-flash-ai-model-is-now-stable-replacing-1-5-pro-in-gemini-app#:~:text=The%20Google%20Gemini%202,Google%20in%20a%20blog%20post))The Mobile Indian (2025) *“Gemini 2.0 Flash…replacing 1.5 Pro”*  
 ([Gemini models  |  Gemini API  |  Google AI for Developers](https://ai.google.dev/gemini-api/docs/models#:~:text=Gemini%202.0%20Flash,volume%20and%20lower%20intelligence%20tasks))Google (2025) *Gemini API docs – “Flash-Lite optimized for cost efficiency”*  
 ([Gemini models  |  Gemini API  |  Google AI for Developers](https://ai.google.dev/gemini-api/docs/models#:~:text=Imagen%203%20%20%60imagen,bidirectional%20voice%20and%20video%20interactions))Google (2025) *Gemini API docs – “Flash Live for voice interactions”*  
 ([DeepSeek - Wikipedia](https://en.wikipedia.org/wiki/DeepSeek#:~:text=On%2020%20January%202025%2C%20DeepSeek,drop%20in%20Nvidia%27s%20share))Wikipedia (2025) “DeepSeek – R1 model launched”  
 ([DeepSeek - Wikipedia](https://en.wikipedia.org/wiki/DeepSeek#:~:text=The%20DeepSeek,significantly%20lower%20than%20other%20LLMs))Wikipedia (2025) “DeepSeek – R1 comparable to GPT-4, low training cost”  
 ([Grok (chatbot) - Wikipedia](https://en.wikipedia.org/wiki/Grok_(chatbot)#:~:text=On%20February%2017%2C%202025%2C%20xAI,3%20was%20trained%20with)) ([Grok (chatbot) - Wikipedia](https://en.wikipedia.org/wiki/Grok_(chatbot)#:~:text=xAI%20also%20released%20Grok,41))Wikipedia (2025) “Grok 3 – xAI’s latest model release and Grok-3 Mini”  
 ([Grok (chatbot) - Wikipedia](https://en.wikipedia.org/wiki/Grok_(chatbot)#:~:text=Initially%2C%20access%20to%20Grok,41))Wikipedia (2025) “Grok 3 availability – limited to Premium+ and plans for API”

## 最新LLM機能対応比較表（2025年4月時点）

ありがとうございます。すべての項目について、2025年4月時点での最新情報を各プロバイダーの公式サイト・ドキュメントを元に確認し、比較表を作成します。

新たに「アーティファクト類似機能」の項目も追加し、Claudeの「Artifacts」機能に準ずる表示・編集機能がChatGPTやGeminiにあるかどうかも明記します。完了次第、表形式でご報告いたします。

## 最新LLM機能対応比較表（2025年4月時点）

| 機能             | **OpenAI – ChatGPT** (GPT-3.5/GPT-4 系列) | **Anthropic – Claude** (Claude 3 ファミリ) | **Google – Gemini** (Bard/Gemini モデル) |
|------------------|--------------------------------------|---------------------------------------|--------------------------------------|
| **モバイルアプリ** | **公式アプリあり**（iOSは2023年5月公開、Androidは2023年7月公開）。無料ユーザー含め利用可能。<br>Chat履歴同期や音声チャット等に対応。 | **公式アプリあり**（iOS/Android両対応）。2024年に公式モバイルアプリをリリース。無料プランでも利用可能。チャット履歴の同期やファイル添付（画像・PDF等）に対応。 | **公式アプリあり**（Android中心）。2024年2月より Bard を **Gemini** としてリブランドし、新たにモバイルアプリを提供。AndroidではGoogleアシスタントに統合され、iOS版GoogleアシスタントもGemini対応＊。 |
| **音声認識** <br>(音声入力/出力) | **音声入力・出力対応**：ChatGPTは音声会話モードを提供（Plus/Enterpriseユーザーが利用可能）。Whisperによる高精度な音声認識でマイク入力ができ、5種類のAI音声での読み上げも可能。モバイル版では設定から音声会話を有効化しヘッドホンボタンで開始。2025年にはWeb版でも音声チャット開始に対応。 | **音声入力対応、音声出力は未提供**：モバイルアプリでマイクアイコンから音声によるプロンプト入力（Dictation機能）が可能（無料含む全プラン利用可）。対応言語は日本語を含む十数言語。音声入力はテキスト化後すぐ削除されプライバシー保護。※生成内容の音声読み上げ機能は**2025年4月時点未実装**（将来提供予定の報道あり）。 | **音声入力・出力対応**：従来のGoogleアシスタント同様、「OK Google」やマイクからGeminiに話しかけ可能。ユーザーの声を認識し、応答は音声でも返答（従来Assistantの音声合成エンジンを継承）。スマホの設定でGeminiの音声応答オン/オフ切替可能＊。自然な対話型のハンズフリー操作に強み。 |
| **WEB検索** <br>(インターネットから最新情報取得) | **「Browse with Bing」機能**によりウェブ検索が可能（ChatGPT Plus/Enterpriseで利用可。設定のBeta機能で有効化後、GPT-4モードで選択）。Bing検索経由で2021年以降の最新情報を取得し、回答に引用付きの出典リンクを含める。2023年9月にPlusユーザー向け正式提供開始。＊Freeプランには順次拡大中（2024年に一部開放）。 | **「Claudeのウェブ検索」機能**：Claude 3.7 (Sonnet)モデルで**インターネット検索**に対応（有料プランで2025年3月からプレビュー提供、設定で有効化） ([Claude can now search the web \ Anthropic](https://www.anthropic.com/news/web-search#:~:text=You%20can%20now%20use%20Claude,from%20the%20most%20recent%20data)) ([Claude can now search the web \ Anthropic](https://www.anthropic.com/news/web-search#:~:text=Getting%20started))。プロンプト内容に応じてClaudeが自動で複数の検索クエリを実行し、**最新情報を引用付きで回答** ([Claude can now search the web \ Anthropic](https://www.anthropic.com/news/web-search#:~:text=relevant%20responses,from%20the%20most%20recent%20data))。米国の有料ユーザーから提供開始し、無料ユーザーや他国へ拡大予定 ([Claude can now search the web \ Anthropic](https://www.anthropic.com/news/web-search#:~:text=Web%20search%20is%20available%20now,web%20to%20inform%20its%20response))。 | **常時インターネット接続**：Bard/Geminiは当初からGoogle検索と連動し、最新のウェブ情報に基づいて回答。回答中の重要語句に下線が引かれ、クリックすると検索結果から裏付け情報を表示する「**ダブルチェック**」機能あり（2023年9月追加）。ユーザーが「Google検索で確認」を選ぶことで出典を参照可能。 |
| **文章編集機能** <br>(テキストリライト・スタイル変更など) | **明示的なUI機能は無し**。プロンプト指示により文章の言い換え・校正は可能だが、**ワンクリックでのトーン変更機能は提供されていない**。ChatGPTはユーザー自身がプロンプトを編集再送信することも可能（送信済みユーザーメッセージの手動編集機能あり）。 | **明示的なUI機能は無し**。長文の要約・リライトなど高度な編集指示に応答可能。最大100kトークンの長文も扱えるため、大規模文章の推敲に強み。出力結果を再編集したい場合も追加の指示で対応可。＊UI上でワンクリック変更する機能はないが、Claudeの回答内容を**Artifacts**（後述）で編集・バージョン管理することは可能。 | **トーン/スタイル変更オプション提供**：Bardは**5種類の口調設定**（「簡潔に」「詳細に」「プロフェッショナルに」等）をボタン操作で適用可能 ([Google Bard Gets a Major Update · ChatGPT Users - Skool](https://www.skool.com/chatgpt/google-bard-gets-a-major-update#:~:text=Google%20Bard%20Gets%20a%20Major,Bard%20features%20to%20help))。2023年7月のアップデートで導入され、回答後にトーンを切り替えて再生成できます。また、生成テキストをGoogleドキュメントへワンクリックでエクスポートし、さらなる編集も可能。 |
| **データ分析** <br>(ファイル解析・コード実行) | **「Advanced Data Analysis」機能**（旧名Code Interpreter）としてGPT-4 Plusで提供。ユーザーはCSVやExcel、JSON、PDF等の**データファイルを最大25個アップロード**し、ChatGPTがPythonコードを生成・実行して分析。グラフ作成や統計計算も自動化され、対話しながら**インタラクティブな表やチャート**を表示・カスタマイズ可能。分析過程のPythonコードや結果も確認・ダウンロードできる。 | **テキストベースで対応（コード実行環境なし）**。Claudeは大容量コンテキストを活かし、貼り付けられたデータ（表やCSVテキスト等）から洞察を引き出すことが可能。複雑な計算や変換も試みるが、**専用のコード実行エンジンは提供されていない**ため、ChatGPTほど高精度なデータ解析・可視化は困難。＊現在、Claude自身がスプレッドシート等を計算処理する機能はなく、必要に応じてユーザーが手動で結果を検証する形。 | **コード実行による分析補助**：Geminiは**暗黙的なコード実行**機能により、数値計算や文字列操作を内部でPythonコードとして実行し精度向上。Bardに表データや計算問題を与えると裏側でコードを走らせて解を導く。専用のユーザー向け解析モードはないが、Googleスプレッドシートとの連携が強み。**「Sheetsにエクスポート」**機能で表を直接スプレッドシート化でき、ユーザーはそのままシート上で編集・分析を続行可能。 |
| **画像説明** <br>(画像入力解析) | **GPT-4 Vision**により画像理解が可能（ChatGPT Plus/EnterpriseのGPT-4モードで提供）。ユーザーが画像をアップロードし質問すると、内容を詳細に描写・分析したり、写っている文章や物体を認識できる。複数画像にも対応し、一部をマークして質問することも可能（モバイルアプリでは画像にペンで注釈指示が可能）。 | **画像入力解析に対応**。Claude 3 ファミリは**視覚情報の理解が可能**で、ユーザーが画像ファイルを添付するとその内容を説明・分析 ([AI Eclipse Vision: How Claude 3 Sees the Sun - Mission Cloud](https://www.missioncloud.com/blog/image-analysis-with-claude-3-the-solar-eclipse#:~:text=Cloud%20www,with%20remarkable%20depth%20and%20nuance)) ([GPT4o-mini is better at reading images than Claude 3.5 Sonnet](https://www.reddit.com/r/ClaudeAI/comments/1f6o2ry/gpt4omini_is_better_at_reading_images_than_claude/#:~:text=I%20think%20that%20it%20depends,tables%20from%20documents%20and%20reports))。公式iOSアプリでも写真を送信してリアルタイム分析できる旨が紹介されている。図表や手書き文字の読み取りも得意との評価あり。＊API経由でも画像（バイナリ）をメッセージとして送信可能。（企業向けにはPDF内の画像や図の解析需要にも対応）。 | **画像入力解析に対応**。Bard/GeminiはGoogleレンズと統合されており、画像をアップするとその内容を理解して対話可能。例えば写真に写る犬の品種を尋ねたり、手書きメモの内容を読み取って要約する、といったことができる。多言語OCR能力も備え、得られたテキストや描画内容を元に回答する。※画像アップロード機能は2023年に追加。 |
| **画像生成** <br>(テキスト→画像生成) | **対応**（DALL·E 3 統合）：ChatGPTのGPT-4モードでテキストから画像を生成可能。2023年10月にPlus/Enterprise向け提供開始。プロンプトに応じて複数枚の画像を提案し、ユーザーが選択・リトライできる。モデルはOpenAIの最新画像生成AI *DALL·E 3* を使用。※ChatGPT無料版も2024年以降、1日あたり2枚までの利用枠で順次対応。 | **未対応**。Anthropic Claudeには**画像生成機能は提供されていない**（2025年4月時点）。テキストから直接画像を作成することはできず、外部の画像生成サービスとの連携も公式には行っていない。 | **対応**（Imagen 2 統合）：Bard（Gemini）は2024年2月よりテキスト→画像生成を無料提供開始。Google DeepMind開発の画像生成モデル*Imagen 2*により、「サーフボードに乗る犬の絵」のような指示で多彩な画像を作成。生成画像には識別用のデジタル透かし（SynthID）が埋め込まれ、不適切画像や実在人物の生成はフィルタリングされる。 |
| **動画生成** <br>(テキスト→動画生成) | **未対応**。ChatGPT/OpenAIは動画生成を行う公式機能を持たない（研究段階）。ユーザーがテキストから直接動画を得ることはできない。 | **未対応**。（Anthropic Claudeに動画生成機能はない。） | **限定対応**：**Gemini Advanced**（有料のGoogle One AIプレミアム会員向け）で短いビデオ生成が可能。2025年4月に提供開始。最新動画モデル*Veo 2*を用い、8秒程度・解像度720pの映像をMP4形式で生成。プロンプトに沿ったシネマティックな動画シーンを作り出せる。生成結果はモバイルから直接YouTube Shorts等に共有可能。＊月あたり生成本数に上限あり。 |
| **DeepResearch**<br>(自動深層リサーチ) | **提供あり**：「*ChatGPT Deep Research*」エージェントを2025年2月に導入。ChatGPTがユーザーの曖昧な要求から**自律的にマルチステップ調査**を実行し、数十～数百のオンライン情報源を精査してレポートを生成。OpenAIの新モデルo3を最適化したモードで動作し、**回答には引用を明示**。当初Proプラン向け開始、2025年2月末よりPlusユーザーにも拡大。 | **提供あり**：Claudeの**「Research」機能**。2025年4月発表の機能で、Claudeがユーザーに代わり**複数の検索クエリを自動実行し**、関連情報を総合して回答する。質問の角度を変えながら段階的に調査し、網羅的な結果を短時間で提供。回答には出典となるURLの引用が含まれ信頼性を担保。社内文書（Google Workspace連携）とウェブ双方を横断検索可能。 | **（該当モード無し）**：GeminiにはChatGPT Deep Researchのような専用の自動調査モードはありません。標準の対話内でインターネット検索は行いますが、ユーザーの要求に対し裏で長時間かけてレポートをまとめる機能は未提供です。※回答内容の確認には前述の「ダブルチェック」機能で都度検索結果を参照する形となります。 |
| **コード支援** <br>(コーディング能力・補助機能) | **高度に対応**：GPT-4は多くのコーディングタスクで高性能を発揮（SWEベンチマークで大幅改善）。ChatGPT Plusでは**コード解釈モード**でPython実行環境が使え、データ処理やファイル操作を自動化。またOpenAI APIでは関数呼び出し機能によりプラグイン経由でツール操作も可能。2025年4月公開のGPT-4.1モデルではコーディング性能がさらに向上。 | **高度に対応**：Claude 3.5/3.7モデルは**ソフトウェア開発に長けた設計**で、公開ベンチマークでも高水準（SWEベンチ正答率49.0%で同等モデル中トップ）。大容量コンテキストにより長いコードも保持。2024年10月にClaude 3.5で**「コンピュータ操作 (Computer Use)」機能**をβ提供開始し、Claudeが仮想画面上でエディタやターミナルを操作する形で多段階のコーディングも可能にした（Replitなどが活用）。 | **高度に対応**：**Gemini Ultra**はコーディングベンチマークでGPT-4を上回る性能を示し、高度なコード生成・デバッグに対応。20以上の言語でコードを書き説明できる。2023年6月には**暗黙のコード実行**技術で数学・プログラミング問答の正確性が向上。さらに2024年以降、Gemini Advancedでは対話中にPythonコードを作成・実行して結果を返す機能も実装され（Google AI StudioやAPI経由で利用可能）＊。 |
| **ブラウザ操作** <br>(ウェブUIナビゲーション) | **限定的**：ChatGPT自体に任意サイトのGUI操作機能は無い。ウェブ閲覧は可能だが（前述のBrowse機能）、フォーム入力やボタン操作といった**対話外でのブラウザ自動操作は不可**。※ただし、拡張機能やプラグインを介して特定のウェブサービスと連携させることは可能（公式Plugins機能でブラウザ操作系のプラグインを利用する等）。 | **提供あり（β）**：Claude 3.5 Sonnetは**「コンピュータ利用」機能**で仮想ブラウザを操作可能。開発者向けAPIでPuppeteer等と連携し、Claudeが自動で画面上のリンククリック・スクロール・入力を行える。Claude DesktopアプリではMCP経由でローカルファイルやブラウザにアクセスし、ファイル管理やWeb閲覧を補助することも可能。＊この機能は実験的で不安定な場合あり。 | **未対応**：Gemini（Bard）はウェブ情報の取得は行うものの、ユーザーの代わりに任意のウェブサイト上でボタンをクリックしたりフォーム入力するといった**ブラウザRPA機能は提供されていない**。（スマホにおけるアプリ起動や情報カード表示程度には対応するが、汎用的なブラウザ操作エージェント機能は無し。） |
| **MCP対応** <br>(モデルコンテキストプロトコル) | **非対応**：ChatGPTはOpenAI独自のプラグイン機構や関数呼び出しで外部ツール連携を提供しているが、Anthropic提唱の**Model Context Protocol (MCP)** はサポートしていない。プラグインは各サービスごとに個別対応が必要。 | **対応**：Anthropicは2024年11月に**MCPをオープンソース公開**。Claude 3.5以降はMCPクライアント/サーバーを介したデータソース接続が可能で、Google DriveやSlack、GitHub等の既成MCPコネクタが提供。開発者は自社データをMCPサーバーとして公開し、ClaudeをMCPクライアントとして接続できる。 | **非対応**：（Gemini向けにAnthropicのMCPのような共通プロトコルは提供されていない。）Googleは独自APIで外部ツール連携（例：コード実行や関数呼び出し）を実装しているが、開発者が共通仕様でデータ接続する仕組みは特に公開していない。 |
| **Google連携** <br>(Googleサービス統合) | **限定的**：ChatGPT自体にGoogleサービスとの直接統合はないが、**ファイルアップロード機能でGoogle Driveからの直接インポート**に対応（2024年5月～、Drive経由でシートやドキュメントを追加可能）。それ以外（Gmail/カレンダー連携等）は未対応。（※但しユーザーがプラグインを利用してGoogleサービスへ接続することは可能。） | **対応**：**Google Workspace連携**機能を提供。2025年4月より、ClaudeにGoogleアカウントを接続することで **Gmail・Googleカレンダー・Googleドキュメントの内容を検索・利用可能**。メール内容や予定表を参照して要約・アクションアイテム抽出を行ったり、Drive上の文書を横断検索して回答に反映することができる。 | **高度に対応**：Gemini (Bard) は**Gmail、Googleドライブ、Docs、マップ、YouTube**など主要サービスとシームレスに連携。2023年9月に導入された**「拡張機能 (Extensions)」**により、ユーザーの許可のもとBardが各サービスから必要情報を自動取得できる。例えば「Drive上の〇〇というPDFを要約して」と頼めばDrive内検索〜内容要約まで実行。返信内容を直接Googleアプリ（Gmail下書きやDocs）に送ることも可能。 |
| **Microsoft連携** <br>(Microsoftサービス統合) | **部分的にあり**：ChatGPTは**Bing検索エンジン（Microsoft）**を組み込みブラウズ機能として利用。また、MicrosoftはOpenAI技術を自社製品に統合済み（例：Bing ChatやMicrosoft 365 CopilotはGPT-4を搭載）。Azure OpenAI経由でChatGPT機能を組み込むことも可能。＊ただしChatGPTクライアントから直接OutlookやTeams等のMicrosoftユーザーサービスに接続する機能は提供されていない。 | **ほぼ無し**：Anthropic ClaudeとMicrosoft製品の直接統合は**公式には発表されていない**。主要クラウドパートナーはGoogle CloudおよびAWS (Bedrock) であり、Microsoft Azure上でClaudeを利用する一般提供も現状なし。 | **無し**：（GeminiはGoogle提供のアシスタントであり、Microsoftのサービスとの直接的な連携機能は持たない。） |
| **PDFサポート** <br>(PDF読み込み) | **対応**：ChatGPTのデータ分析モードで**PDFを直接アップロード**し解析可能。テキスト抽出や要約、表の読み取りも自動で行う。最大25ファイルまで同時アップロードでき、合計容量は約100MB程度まで（Plus/Enterpriseの制限）。 *（通常モードではPDFをそのまま読ませることはできず、内容をコピーペーストする必要あり）*。 | **対応**：Claudeは**PDFファイルのアップロード解析**に対応。Claude.aiやSlack用Claudeでは最大10MBのファイルを添付可能で ([Uploading Files to Claude AI - File Types & Images](https://claudeaihub.com/uploading-files-and-images-to-claude-ai/#:~:text=Uploading%20Files%20to%20Claude%20AI,5%20files%20at%20a%20time))、PDFのテキスト内容を要約・質疑応答できる。大量ページのPDFもClaudeの長大コンテキストで扱いやすく、企業利用では社内PDF資料を丸ごと読み込ませるケースもある。 | **対応**：Bardの**Google Drive拡張**により、ユーザーのGoogle Drive上のPDFを開いて内容を読む・要約することが可能。例えば「Driveの◯◯というPDFを要約して」で、そのPDFを検索しテキスト抽出・要約。Drive連携なしでも、テキスト化したPDF内容を貼り付ければ分析可能。 |
| **バッチ処理** <br>(大量クエリ一括実行) | **明確な提供なし**：ChatGPT UIにおける一括バッチ実行機能はない。API利用時も、一リクエストに一対話（ストリーム処理）という形であり、**複数プロンプトを非同期一括処理する公式機能は未提供**。＊（2023年以降、OpenAIはファイルベースのデータ入力やファインチューニングAPIを提供しているが、Anthropicのような汎用バッチ実行APIは存在しない。） | **対応（API）**：2023年より**「メッセージバッチAPI」**を提供 ([Introducing the Message Batches API - Anthropic](https://www.anthropic.com/news/message-batches-api#:~:text=We%27re%20introducing%20a%20new%20Message,large%20volumes%20of%20queries%20asynchronously))。開発者が最大数万件のプロンプトをまとめて送信し、非同期にコスト効率よく処理可能 ([Introducing the Message Batches API - Anthropic](https://www.anthropic.com/news/message-batches-api#:~:text=We%27re%20introducing%20a%20new%20Message,large%20volumes%20of%20queries%20asynchronously))。結果はJSONLファイルで一括取得。社内検証では大規模バッチほど単位コストが割安になるメリットあり。※Claude.aiなどUI上でのバッチ投入機能は無し。 | **対応（API）**：Google CloudのVertex AI上で**Geminiモデルのバッチ予測**が可能。BigQueryテーブルやCloud Storage上の大量データを入力とし、一括で複数プロンプトを推論する仕組み。出力もファイルにまとめて保存。開発用途でレイテンシ非重視の大量処理に適する。一方、一般ユーザー向けのBardでは対話をまたいだバッチ処理機能は提供されていない。 |
| **アーティファクト類似機能** <br>(生成物の分離表示・編集UI) | **未対応**：ChatGPTの回答はすべて通常のチャット画面にテキストとして表示され、コードや長文もインラインで提供される。**出力をサブウィンドウで編集・履歴管理する機能はない**。（※ChatGPT Plusではグラフや画像生成結果を拡大表示することは可能。また2024年末導入の「Canvas」は図表やテキストをキャンバス上に配置できるが、対話出力を独立ウィンドウ化するものではない。） | **対応**：Claudeは**「Artifacts」**機能で大きな生成コンテンツをチャットとは別ウィンドウに分離表示。15行超のコードブロックや文章・図表は自動的にArtifact化され、右側ペインで見やすく編集可能。ユーザーはArtifact内で内容修正を指示でき、バージョン履歴も確認可能。複数Artifactを同時に開き参照することも可能。生成コンテンツを後で再利用したり、コピペ・ダウンロードするのが容易になる。 | **未対応**：Gemini（Bard）にはClaudeのArtifactsに相当する**専用表示フレームは存在しない**。回答は常にチャット画面上にテキストとして表示される。必要に応じてユーザーが内容を選択コピーしたり、「エクスポート」機能でDocsやSheetsに移すことはできるが、チャットUI内で独立編集できるパネル等は提供されていない。 |

> **注:** 「Gemini」はGoogle Bardの後継モデル群の総称です（2023年末～2024年にかけて順次アップデート）。上記で「Gemini Ultra/Pro/Nano」と記載した場合、Ultraは最大モデル、Proは汎用高性能モデル、Nanoはモバイルデバイス向け軽量モデルを指します。Gemini AdvancedとはUltra 1.0モデルを一般提供する上位プランの名称です。 ChatGPTのGPT-4oやGPT-4.1は2024～2025年時点の最新GPT-4シリーズモデルです。

